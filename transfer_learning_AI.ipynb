{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b655f09b-7c91-4b6d-ac13-7203a360903e",
   "metadata": {
    "id": "b655f09b-7c91-4b6d-ac13-7203a360903e"
   },
   "source": [
    "# training a pre trained model with data for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b2f6a9-de86-45d8-8a4b-1365d830bd39",
   "metadata": {
    "id": "81b2f6a9-de86-45d8-8a4b-1365d830bd39"
   },
   "source": [
    "* Glue -- sst2 dataset\n",
    "* train and tests dataset\n",
    "* ver interesting table: sentence - tokens - ids - n_tokens - essential_tokens\n",
    "* predicting labels from other datasets different fron sst2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e464377d-8975-4cb3-97c1-d9569eae5b26",
   "metadata": {
    "id": "e464377d-8975-4cb3-97c1-d9569eae5b26"
   },
   "source": [
    "# libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab3afc5b-e646-40ff-b30c-54a0c39aefa3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30520,
     "status": "ok",
     "timestamp": 1725026930710,
     "user": {
      "displayName": "jcdata",
      "userId": "10219721298441697414"
     },
     "user_tz": -120
    },
    "id": "ab3afc5b-e646-40ff-b30c-54a0c39aefa3",
    "outputId": "925e325b-bf13-41af-cd90-1e548d256170",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets transformers evaluate torch scikit-learn accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05cb47a-d04a-4c50-9847-b2710da3eb8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f356f062-ed4b-4260-ae96-6967f82ab1b5",
   "metadata": {
    "id": "f356f062-ed4b-4260-ae96-6967f82ab1b5",
    "outputId": "2c853a43-b22c-402a-a629-e11e31dd6ef9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bda474-c51f-418e-aa20-0957f2d8be71",
   "metadata": {
    "id": "42bda474-c51f-418e-aa20-0957f2d8be71",
    "tags": []
   },
   "source": [
    "# importing the model - tokenizer - dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d04084c-b880-44f0-ad0c-1fcf19b353ad",
   "metadata": {
    "id": "9d04084c-b880-44f0-ad0c-1fcf19b353ad",
    "outputId": "0f3dcdf7-4c47-4fbf-be45-af46a932b2cc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 'num_labels=2' specifies that this is a binary classification task\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2)\n",
    "# DistilBERT is a smaller, faster version of BERT. It has already been\n",
    "# pre-trained on general language tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b0dd2fd-cfc3-4195-a010-a60bb6eeac23",
   "metadata": {
    "id": "2b0dd2fd-cfc3-4195-a010-a60bb6eeac23",
    "outputId": "899adae5-aa5a-4c97-db6a-53edadaf52e9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer for DistilBERT (or any other model to fine-tune).\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8a2f99f-560b-42ca-9f56-3e3e353452a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m     \n",
       "\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTruncationStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           DistilBertTokenizerFast\n",
       "\u001b[0;31mString form:\u001b[0m   \n",
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_lengt <...> Token(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "           }\n",
       "\u001b[0;31mLength:\u001b[0m         30522\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.conda/envs/default/lib/python3.9/site-packages/transformers/models/distilbert/tokenization_distilbert_fast.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Construct a \"fast\" DistilBERT tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.\n",
       "\n",
       "This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n",
       "refer to this superclass for more information regarding those methods.\n",
       "\n",
       "Args:\n",
       "    vocab_file (`str`):\n",
       "        File containing the vocabulary.\n",
       "    do_lower_case (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to lowercase the input when tokenizing.\n",
       "    unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n",
       "        The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
       "        token instead.\n",
       "    sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n",
       "        The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n",
       "        sequence classification or for a text and a question for question answering. It is also used as the last\n",
       "        token of a sequence built with special tokens.\n",
       "    pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n",
       "        The token used for padding, for example when batching sequences of different lengths.\n",
       "    cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n",
       "        The classifier token which is used when doing sequence classification (classification of the whole sequence\n",
       "        instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
       "    mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n",
       "        The token used for masking values. This is the token used when training this model with masked language\n",
       "        modeling. This is the token which the model will try to predict.\n",
       "    clean_text (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to clean the text before tokenization by removing any control characters and replacing all\n",
       "        whitespaces by the classic one.\n",
       "    tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see [this\n",
       "        issue](https://github.com/huggingface/transformers/issues/328)).\n",
       "    strip_accents (`bool`, *optional*):\n",
       "        Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n",
       "        value for `lowercase` (as in the original BERT).\n",
       "    wordpieces_prefix (`str`, *optional*, defaults to `\"##\"`):\n",
       "        The prefix for subwords.\n",
       "\u001b[0;31mCall docstring:\u001b[0m\n",
       "Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
       "sequences.\n",
       "\n",
       "Args:\n",
       "    text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
       "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
       "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
       "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
       "    text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
       "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
       "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
       "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
       "    text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
       "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
       "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
       "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
       "    text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
       "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
       "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
       "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
       "\n",
       "    add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
       "        `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
       "        automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
       "        automatically.\n",
       "    padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
       "        Activates and controls padding. Accepts the following values:\n",
       "\n",
       "        - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
       "          sequence if provided).\n",
       "        - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
       "          acceptable input length for the model if that argument is not provided.\n",
       "        - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
       "          lengths).\n",
       "    truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
       "        Activates and controls truncation. Accepts the following values:\n",
       "\n",
       "        - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
       "          to the maximum acceptable input length for the model if that argument is not provided. This will\n",
       "          truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
       "          sequences (or a batch of pairs) is provided.\n",
       "        - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
       "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
       "          truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
       "        - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
       "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
       "          truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
       "        - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
       "          greater than the model maximum admissible input size).\n",
       "    max_length (`int`, *optional*):\n",
       "        Controls the maximum length to use by one of the truncation/padding parameters.\n",
       "\n",
       "        If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
       "        is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
       "        length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
       "    stride (`int`, *optional*, defaults to 0):\n",
       "        If set to a number along with `max_length`, the overflowing tokens returned when\n",
       "        `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
       "        returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
       "        argument defines the number of overlapping tokens.\n",
       "    is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
       "        tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
       "        which it will tokenize. This is useful for NER or token classification.\n",
       "    pad_to_multiple_of (`int`, *optional*):\n",
       "        If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
       "        This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
       "        `>= 7.5` (Volta).\n",
       "    return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
       "        If set, will return tensors instead of list of python integers. Acceptable values are:\n",
       "\n",
       "        - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
       "        - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
       "        - `'np'`: Return Numpy `np.ndarray` objects.\n",
       "\n",
       "    return_token_type_ids (`bool`, *optional*):\n",
       "        Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
       "        the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
       "\n",
       "        [What are token type IDs?](../glossary#token-type-ids)\n",
       "    return_attention_mask (`bool`, *optional*):\n",
       "        Whether to return the attention mask. If left to the default, will return the attention mask according\n",
       "        to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
       "\n",
       "        [What are attention masks?](../glossary#attention-mask)\n",
       "    return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
       "        of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
       "        of returning overflowing tokens.\n",
       "    return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to return special tokens mask information.\n",
       "    return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to return `(char_start, char_end)` for each token.\n",
       "\n",
       "        This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
       "        Python's tokenizer, this method will raise `NotImplementedError`.\n",
       "    return_length  (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to return the lengths of the encoded inputs.\n",
       "    verbose (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to print more information and warnings.\n",
       "    **kwargs: passed to the `self.tokenize()` method\n",
       "\n",
       "Return:\n",
       "    [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
       "\n",
       "    - **input_ids** -- List of token ids to be fed to a model.\n",
       "\n",
       "      [What are input IDs?](../glossary#input-ids)\n",
       "\n",
       "    - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
       "      if *\"token_type_ids\"* is in `self.model_input_names`).\n",
       "\n",
       "      [What are token type IDs?](../glossary#token-type-ids)\n",
       "\n",
       "    - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
       "      `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
       "\n",
       "      [What are attention masks?](../glossary#attention-mask)\n",
       "\n",
       "    - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
       "      `return_overflowing_tokens=True`).\n",
       "    - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
       "      `return_overflowing_tokens=True`).\n",
       "    - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
       "      regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
       "    - **length** -- The length of the inputs (when `return_length=True`)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0e4c076-449b-4991-a82b-4ee948bd127a",
   "metadata": {
    "id": "d0e4c076-449b-4991-a82b-4ee948bd127a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482d1ce18adc49d88847fcf50bf28f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the SST-2 dataset from the GLUE benchmark.\n",
    "# The 'sst2' configuration is used for sentiment classification tasks.\n",
    "dataset = load_dataset('glue', 'sst2')\n",
    "\n",
    "# The SST-2 dataset is a binary classification dataset for sentiment\n",
    "# analysis. It contains sentences with labels 0 (negative) and 1 (positive)\n",
    "# the test set has label = -1 wich means it is unlabelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93566cc4",
   "metadata": {
    "id": "93566cc4",
    "outputId": "6fee2f34-a0c7-473f-d529-b32c53e15cab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ce2fd5-08fd-4b33-b478-5ba5f2a39fad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sampling a smaller subset for training and testing\n",
    "n_train = 600   # Number of training samples\n",
    "n_test = 150    # Number of testing samples\n",
    "\n",
    "# Shuffle the training and test datasets and select a subset.\n",
    "#the test dataset is unlabelled: labels = -1, we use the validation set\n",
    "train_dataset = dataset['train'].shuffle(seed=42).select(range(n_train))\n",
    "test_dataset = dataset['validation'].shuffle(seed=17).select(range(n_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4e6168-572e-49f6-bb46-624537eb91fd",
   "metadata": {
    "id": "6d4e6168-572e-49f6-bb46-624537eb91fd"
   },
   "source": [
    "## tokenizing train - test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b6f3946-0401-4371-8bc2-4656184d9c77",
   "metadata": {
    "id": "4b6f3946-0401-4371-8bc2-4656184d9c77",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function to tokenize the sentences in the dataset.\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['sentence'],           # The text field in SST-2 is 'sentence'\n",
    "        padding=\"longest\",              # Pad to the longest sentence\n",
    "        truncation=True,                # Truncate if the sentence is longer than 512 tokens\n",
    "        max_length=512                  # Max token length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdf4c791-b94b-41b7-8b07-5a5b909118d1",
   "metadata": {
    "id": "cdf4c791-b94b-41b7-8b07-5a5b909118d1",
    "outputId": "3967037d-a578-4e63-a35d-9bb0d79036c0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the training dataset\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Tokenize the test dataset\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Explanation:\n",
    "# The tokenizer converts the sentences into token IDs that the\n",
    "# model can process. We use padding and truncation to ensure all\n",
    "# inputs have the same lenght in each batch. The function 'map' applies the\n",
    "# tokenization to all examples in the dataset in batches for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9836d1b2-2161-4285-84c3-50141ec275f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        Dataset\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "Dataset({\n",
       "    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 600\n",
       "})\n",
       "\u001b[0;31mLength:\u001b[0m      600\n",
       "\u001b[0;31mFile:\u001b[0m        ~/.conda/envs/default/lib/python3.9/site-packages/datasets/arrow_dataset.py\n",
       "\u001b[0;31mDocstring:\u001b[0m   A Dataset backed by an Arrow table.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e8d458-daf8-432e-b559-24a845a76505",
   "metadata": {},
   "source": [
    "### showing sentences - tokens  ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d5e7e48-34f5-4dac-94b9-dbc46fff7b7e",
   "metadata": {
    "id": "1d5e7e48-34f5-4dac-94b9-dbc46fff7b7e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the special token ID for padding, usually tokenizer.pad_token_id\n",
    "pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5d049c7-2004-4b3f-964a-e12220f5dd57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        int\n",
       "\u001b[0;31mString form:\u001b[0m 0\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pad_token_id?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77e0b133-bbec-4d74-b8dc-35259640359c",
   "metadata": {
    "id": "77e0b133-bbec-4d74-b8dc-35259640359c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to process the tokenized dataset and extract necessary fields\n",
    "def extract_token_info_with_essential_tokens(example):\n",
    "    # Get the original sentence\n",
    "    original_sentence = example['sentence'] if 'sentence' in example else None\n",
    "\n",
    "    # Get the tokenized sentence by converting token IDs back to tokens\n",
    "    tokenized_sentence = tokenizer.convert_ids_to_tokens(example['input_ids'])\n",
    "\n",
    "    # Count the number of tokens excluding padding\n",
    "    essential_tokens = sum(1 for token_id in example['input_ids'] if token_id != pad_token_id)\n",
    "\n",
    "    # Return original sentence, tokenized sentence, token IDs, total tokens, and essential tokens\n",
    "    return {\n",
    "        'sentence': original_sentence,                       # The original sentence\n",
    "        'tokenized_sentence': \" \".join(tokenized_sentence),  # Tokenized sentence as a string\n",
    "        'token_ids': example['input_ids'],                   # List of token IDs\n",
    "        'num_tokens': len(example['input_ids']),             # Total number of tokens (including padding)\n",
    "        'essential_tokens': essential_tokens                 # Number of tokens excluding padding\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0e64893-90c8-469e-b528-4ac7284cb47b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the extraction function to the already tokenized dataset\n",
    "processed_test = tokenized_test.map(extract_token_info_with_essential_tokens, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e065b506-07d2-4551-a34e-fe110a393239",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        Dataset\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "Dataset({\n",
       "    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask', 'tokenized_sentence', 'token_ids', 'num_tokens', 'essential_tokens'],\n",
       "    num_rows: 150\n",
       "})\n",
       "\u001b[0;31mLength:\u001b[0m      150\n",
       "\u001b[0;31mFile:\u001b[0m        ~/.conda/envs/default/lib/python3.9/site-packages/datasets/arrow_dataset.py\n",
       "\u001b[0;31mDocstring:\u001b[0m   A Dataset backed by an Arrow table.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ce30866-b861-4280-8cf5-78533eafec46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame from the processed dataset\n",
    "test_data = [\n",
    "    {\n",
    "        \"sentence\": ex['sentence'],\n",
    "        \"tokenized_sentence\": ex['tokenized_sentence'], # Tokenized sentence as a string\n",
    "        \"token_ids\": ex['token_ids'],                   #Tokens id\n",
    "        \"essential_tokens\": ex['essential_tokens'],     # Number of tokens without padding\n",
    "        \"num_tokens\": ex['num_tokens'],                 # Total tokens (with padding)\n",
    "    }\n",
    "    for ex in processed_test\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55cf0bf9-2a97-401f-9bcd-debadffa5664",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        list\n",
       "\u001b[0;31mString form:\u001b[0m [{'sentence': 'at least one scene is so disgusting that viewers may be hard pressed to retain the <...> 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'essential_tokens': 37, 'num_tokens': 54}]\n",
       "\u001b[0;31mLength:\u001b[0m      150\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Built-in mutable sequence.\n",
       "\n",
       "If no argument is given, the constructor creates a new empty list.\n",
       "The argument must be an iterable if specified.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba48ac88-6fb6-45d6-99c8-326267cfe222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a pandas DataFrame\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "# Sort the DataFrame by the number of essential tokens in descending order\n",
    "df_test = df_test.sort_values(by=\"essential_tokens\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfcedba6-efda-48ee-ba28-1d62efc6a44c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "      <th>token_ids</th>\n",
       "      <th>essential_tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>for all its technical virtuosity , the film is...</td>\n",
       "      <td>[CLS] for all its technical vi ##rt ##uo ##sit...</td>\n",
       "      <td>[101, 2005, 2035, 2049, 4087, 6819, 5339, 1909...</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>the special effects and many scenes of weightl...</td>\n",
       "      <td>[CLS] the special effects and many scenes of w...</td>\n",
       "      <td>[101, 1996, 2569, 3896, 1998, 2116, 5019, 1997...</td>\n",
       "      <td>47</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>but the power of these ( subjects ) is obscure...</td>\n",
       "      <td>[CLS] but the power of these ( subjects ) is o...</td>\n",
       "      <td>[101, 2021, 1996, 2373, 1997, 2122, 1006, 5739...</td>\n",
       "      <td>47</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>it does nothing new with the old story , excep...</td>\n",
       "      <td>[CLS] it does nothing new with the old story ,...</td>\n",
       "      <td>[101, 2009, 2515, 2498, 2047, 2007, 1996, 2214...</td>\n",
       "      <td>47</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>the tale of tok ( andy lau ) , a sleek sociopa...</td>\n",
       "      <td>[CLS] the tale of to ##k ( andy lau ) , a slee...</td>\n",
       "      <td>[101, 1996, 6925, 1997, 2000, 2243, 1006, 5557...</td>\n",
       "      <td>46</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>big fat waste of time .</td>\n",
       "      <td>[CLS] big fat waste of time . [SEP] [PAD] [PAD...</td>\n",
       "      <td>[101, 2502, 6638, 5949, 1997, 2051, 1012, 102,...</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>it treats women like idiots .</td>\n",
       "      <td>[CLS] it treats women like idiots . [SEP] [PAD...</td>\n",
       "      <td>[101, 2009, 18452, 2308, 2066, 28781, 1012, 10...</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>a deep and meaningful film .</td>\n",
       "      <td>[CLS] a deep and meaningful film . [SEP] [PAD]...</td>\n",
       "      <td>[101, 1037, 2784, 1998, 15902, 2143, 1012, 102...</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>a wildly inconsistent emotional experience .</td>\n",
       "      <td>[CLS] a wildly inconsistent emotional experien...</td>\n",
       "      <td>[101, 1037, 13544, 20316, 6832, 3325, 1012, 10...</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>one from the heart .</td>\n",
       "      <td>[CLS] one from the heart . [SEP] [PAD] [PAD] [...</td>\n",
       "      <td>[101, 2028, 2013, 1996, 2540, 1012, 102, 0, 0,...</td>\n",
       "      <td>7</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  \\\n",
       "13   for all its technical virtuosity , the film is...   \n",
       "34   the special effects and many scenes of weightl...   \n",
       "47   but the power of these ( subjects ) is obscure...   \n",
       "113  it does nothing new with the old story , excep...   \n",
       "54   the tale of tok ( andy lau ) , a sleek sociopa...   \n",
       "..                                                 ...   \n",
       "97                            big fat waste of time .    \n",
       "66                      it treats women like idiots .    \n",
       "109                      a deep and meaningful film .    \n",
       "88       a wildly inconsistent emotional experience .    \n",
       "64                               one from the heart .    \n",
       "\n",
       "                                    tokenized_sentence  \\\n",
       "13   [CLS] for all its technical vi ##rt ##uo ##sit...   \n",
       "34   [CLS] the special effects and many scenes of w...   \n",
       "47   [CLS] but the power of these ( subjects ) is o...   \n",
       "113  [CLS] it does nothing new with the old story ,...   \n",
       "54   [CLS] the tale of to ##k ( andy lau ) , a slee...   \n",
       "..                                                 ...   \n",
       "97   [CLS] big fat waste of time . [SEP] [PAD] [PAD...   \n",
       "66   [CLS] it treats women like idiots . [SEP] [PAD...   \n",
       "109  [CLS] a deep and meaningful film . [SEP] [PAD]...   \n",
       "88   [CLS] a wildly inconsistent emotional experien...   \n",
       "64   [CLS] one from the heart . [SEP] [PAD] [PAD] [...   \n",
       "\n",
       "                                             token_ids  essential_tokens  \\\n",
       "13   [101, 2005, 2035, 2049, 4087, 6819, 5339, 1909...                54   \n",
       "34   [101, 1996, 2569, 3896, 1998, 2116, 5019, 1997...                47   \n",
       "47   [101, 2021, 1996, 2373, 1997, 2122, 1006, 5739...                47   \n",
       "113  [101, 2009, 2515, 2498, 2047, 2007, 1996, 2214...                47   \n",
       "54   [101, 1996, 6925, 1997, 2000, 2243, 1006, 5557...                46   \n",
       "..                                                 ...               ...   \n",
       "97   [101, 2502, 6638, 5949, 1997, 2051, 1012, 102,...                 8   \n",
       "66   [101, 2009, 18452, 2308, 2066, 28781, 1012, 10...                 8   \n",
       "109  [101, 1037, 2784, 1998, 15902, 2143, 1012, 102...                 8   \n",
       "88   [101, 1037, 13544, 20316, 6832, 3325, 1012, 10...                 8   \n",
       "64   [101, 2028, 2013, 1996, 2540, 1012, 102, 0, 0,...                 7   \n",
       "\n",
       "     num_tokens  \n",
       "13           54  \n",
       "34           54  \n",
       "47           54  \n",
       "113          54  \n",
       "54           54  \n",
       "..          ...  \n",
       "97           54  \n",
       "66           54  \n",
       "109          54  \n",
       "88           54  \n",
       "64           54  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6223ae-f687-4a28-8d7b-896143d5bae8",
   "metadata": {
    "id": "3c6223ae-f687-4a28-8d7b-896143d5bae8"
   },
   "source": [
    "## labelling (predicting) with the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b37cce-4f7e-46ad-9d4c-301b43e00d87",
   "metadata": {
    "id": "80b37cce-4f7e-46ad-9d4c-301b43e00d87"
   },
   "source": [
    "### labelling a single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8819d80b-acab-49af-93de-03e60ffab663",
   "metadata": {
    "id": "8819d80b-acab-49af-93de-03e60ffab663",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_label_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Takes a sentence and returns the original sentence, the tokenized sentence,\n",
    "    the token IDs, the softmax probabilities, and the predicted label.\n",
    "\n",
    "    Args:\n",
    "    - sentence (str): The input sentence for sentiment analysis.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing:\n",
    "        - 'original_sentence': The original sentence.\n",
    "        - 'tokenized_sentence': The tokenized version of the sentence.\n",
    "        - 'input_ids': The token IDs (numerical representation).\n",
    "        - 'softmax_probs': The softmax probabilities for each class.\n",
    "        - 'predicted_label': The predicted class label.\n",
    "        - 'sentiment': Sentiment as 'positive' or 'negative'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Tokenize the sentence\n",
    "    tokens = tokenizer(sentence, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=512)\n",
    "\n",
    "    # Step 2: Get the token IDs and tokenized sentence\n",
    "    input_ids = tokens['input_ids']\n",
    "    tokenized_sentence = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # Step 3: Pass the tokenized input to the model to get logits\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        output = model(**tokens)\n",
    "        logits = output.logits\n",
    "\n",
    "    # Step 4: Apply softmax using torch to get probabilities\n",
    "    softmax_probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    # Step 5: Get the predicted label (argmax of softmax output)\n",
    "    predicted_label = torch.argmax(softmax_probs, dim=-1).item()\n",
    "\n",
    "    # Step 6: Determine sentiment based on the predicted label\n",
    "    sentiment_label = \"positive\" if predicted_label == 1 else \"negative\"\n",
    "\n",
    "    # Step 7: Prepare result dictionary\n",
    "    result = {\n",
    "        \"original_sentence\": sentence,\n",
    "        \"tokenized_sentence\": tokenized_sentence,\n",
    "        \"input_ids\": input_ids[0].tolist(),\n",
    "        \"softmax_probs\": softmax_probs[0].tolist(),  # Convert tensor to list\n",
    "        \"predicted_label\": predicted_label,\n",
    "        \"sentiment\": sentiment_label\n",
    "    }\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "183cf205-35cf-480c-8602-75a09a1fa490",
   "metadata": {
    "id": "183cf205-35cf-480c-8602-75a09a1fa490",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence_label = predict_label_sentence(\"I am happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a54f7f32-5935-47f3-b660-da05144a8c43",
   "metadata": {
    "id": "a54f7f32-5935-47f3-b660-da05144a8c43",
    "outputId": "78c08d01-c712-4f32-e83e-6188e1d3a8c5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_sentence': 'I am happy',\n",
       " 'tokenized_sentence': ['[CLS]', 'i', 'am', 'happy', '[SEP]'],\n",
       " 'input_ids': [101, 1045, 2572, 3407, 102],\n",
       " 'softmax_probs': [0.46126559376716614, 0.5387344360351562],\n",
       " 'predicted_label': 1,\n",
       " 'sentiment': 'positive'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a94232-7568-4e2e-bea5-ef50bb27f1d9",
   "metadata": {
    "id": "37a94232-7568-4e2e-bea5-ef50bb27f1d9"
   },
   "source": [
    "### labelling a whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3d3ced7-6b44-4497-b6c7-ac39b9cad1dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_label_dataset(new_data, text_columns=[\"text\", \"sentence\", \"content\", \"title\"]):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a subset of data and returns a DataFrame with\n",
    "    all the sentences, true labels, predicted labels, number of tokens,\n",
    "    softmax probabilities, tokenized sentences, and input IDs, along with accuracy.\n",
    "\n",
    "    Args:\n",
    "    - new_data: The subset of the dataset to evaluate (already selected).\n",
    "    - text_columns: A list of possible text columns to use (default: ['text', 'sentence', 'content', 'title']).\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame containing detailed prediction information.\n",
    "    - accuracy: Accuracy of the model on the dataset (None if labels are unavailable).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Find the appropriate text column\n",
    "    for col in text_columns:\n",
    "        if col in new_data.column_names:\n",
    "            text_column = col\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(f\"None of the specified text columns {text_columns} were found in the dataset.\")\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        \"Sentence\": [],\n",
    "        \"Tokenized Sentence\": [],\n",
    "        \"Input IDs\": [],\n",
    "        \"Number of Tokens\": [],\n",
    "        \"Softmax Probs\": [],\n",
    "        \"Predicted Label\": [],\n",
    "        \"True Label\": [],\n",
    "        \"Sentiment\": []\n",
    "    }\n",
    "    \n",
    "    # Iterate through each example in the dataset with a progress bar\n",
    "    for i in tqdm(range(len(new_data)), desc=\"Labelling Sentences\"):\n",
    "        sentence = new_data[i][text_column]\n",
    "        true_label = new_data[i].get('label', -1)  # Use -1 if label is missing\n",
    "        \n",
    "        # Use predict_label_sentence function to get predictions and other details\n",
    "        prediction = predict_label_sentence(sentence)\n",
    "        \n",
    "        # Count non-padding tokens\n",
    "        input_ids = prediction[\"input_ids\"]\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "        num_tokens = sum([1 for token_id in input_ids if token_id != pad_token_id])\n",
    "        \n",
    "        # Append the details to the results dictionary\n",
    "        results[\"Sentence\"].append(prediction[\"original_sentence\"])\n",
    "        results[\"Tokenized Sentence\"].append(prediction[\"tokenized_sentence\"])\n",
    "        results[\"Input IDs\"].append(prediction[\"input_ids\"])\n",
    "        results[\"Number of Tokens\"].append(num_tokens)\n",
    "        results[\"Softmax Probs\"].append(prediction[\"softmax_probs\"])\n",
    "        results[\"Predicted Label\"].append(prediction[\"predicted_label\"])\n",
    "        results[\"True Label\"].append(true_label)\n",
    "        results[\"Sentiment\"].append(prediction[\"sentiment\"])\n",
    "    \n",
    "    # Convert results to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Check if true labels are available (i.e., not all -1)\n",
    "    if df[\"True Label\"].isin([-1]).all():\n",
    "        accuracy = None\n",
    "    else:\n",
    "        # Calculate Accuracy\n",
    "        accuracy = accuracy_score(df[\"True Label\"], df[\"Predicted Label\"])\n",
    "        \n",
    "    # Return the DataFrame and accuracy\n",
    "    return df, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d818af2b-c43c-47d7-a362-d51bf8c79476",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labelling Sentences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:09<00:00, 15.71it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test_non_trained, accuracy_test_non_trained = predict_label_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "200e6c87-4287-46af-a100-4152cba122e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5066666666666667"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_test_non_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26e85dc6-3765-47fa-902c-ef8d4c3175a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tokenized Sentence</th>\n",
       "      <th>Input IDs</th>\n",
       "      <th>Number of Tokens</th>\n",
       "      <th>Softmax Probs</th>\n",
       "      <th>Predicted Label</th>\n",
       "      <th>True Label</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>at least one scene is so disgusting that viewe...</td>\n",
       "      <td>[[CLS], at, least, one, scene, is, so, disgust...</td>\n",
       "      <td>[101, 2012, 2560, 2028, 3496, 2003, 2061, 1942...</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.49043747782707214, 0.5095624923706055]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>even the finest chef ca n't make a hotdog into...</td>\n",
       "      <td>[[CLS], even, the, finest, chef, ca, n, ', t, ...</td>\n",
       "      <td>[101, 2130, 1996, 10418, 10026, 6187, 1050, 10...</td>\n",
       "      <td>44</td>\n",
       "      <td>[0.4584480822086334, 0.541551947593689]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>collateral damage finally delivers the goods f...</td>\n",
       "      <td>[[CLS], collateral, damage, finally, delivers,...</td>\n",
       "      <td>[101, 24172, 4053, 2633, 18058, 1996, 5350, 20...</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.47445327043533325, 0.525546669960022]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exciting and direct , with ghost imagery that ...</td>\n",
       "      <td>[[CLS], exciting, and, direct, ,, with, ghost,...</td>\n",
       "      <td>[101, 10990, 1998, 3622, 1010, 2007, 5745, 134...</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.443885862827301, 0.5561141967773438]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and when you 're talking about a slapstick com...</td>\n",
       "      <td>[[CLS], and, when, you, ', re, talking, about,...</td>\n",
       "      <td>[101, 1998, 2043, 2017, 1005, 2128, 3331, 2055...</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.4730995297431946, 0.5269004702568054]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>it 's a bit disappointing that it only manages...</td>\n",
       "      <td>[[CLS], it, ', s, a, bit, disappointing, that,...</td>\n",
       "      <td>[101, 2009, 1005, 1055, 1037, 2978, 15640, 200...</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.4603593051433563, 0.5396407246589661]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>a breezy romantic comedy that has the punch of...</td>\n",
       "      <td>[[CLS], a, bree, ##zy, romantic, comedy, that,...</td>\n",
       "      <td>[101, 1037, 21986, 9096, 6298, 4038, 2008, 203...</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.46655434370040894, 0.5334456562995911]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>the film tries too hard to be funny and tries ...</td>\n",
       "      <td>[[CLS], the, film, tries, too, hard, to, be, f...</td>\n",
       "      <td>[101, 1996, 2143, 5363, 2205, 2524, 2000, 2022...</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.4715810716152191, 0.5284189581871033]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>thanks to scott 's charismatic roger and eisen...</td>\n",
       "      <td>[[CLS], thanks, to, scott, ', s, charismatic, ...</td>\n",
       "      <td>[101, 4283, 2000, 3660, 1005, 1055, 23916, 507...</td>\n",
       "      <td>35</td>\n",
       "      <td>[0.4877658486366272, 0.512234091758728]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>drops you into a dizzying , volatile , pressur...</td>\n",
       "      <td>[[CLS], drops, you, into, a, dizzy, ##ing, ,, ...</td>\n",
       "      <td>[101, 9010, 2017, 2046, 1037, 14849, 2075, 101...</td>\n",
       "      <td>37</td>\n",
       "      <td>[0.47137606143951416, 0.5286239385604858]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  \\\n",
       "0    at least one scene is so disgusting that viewe...   \n",
       "1    even the finest chef ca n't make a hotdog into...   \n",
       "2    collateral damage finally delivers the goods f...   \n",
       "3    exciting and direct , with ghost imagery that ...   \n",
       "4    and when you 're talking about a slapstick com...   \n",
       "..                                                 ...   \n",
       "145  it 's a bit disappointing that it only manages...   \n",
       "146  a breezy romantic comedy that has the punch of...   \n",
       "147  the film tries too hard to be funny and tries ...   \n",
       "148  thanks to scott 's charismatic roger and eisen...   \n",
       "149  drops you into a dizzying , volatile , pressur...   \n",
       "\n",
       "                                    Tokenized Sentence  \\\n",
       "0    [[CLS], at, least, one, scene, is, so, disgust...   \n",
       "1    [[CLS], even, the, finest, chef, ca, n, ', t, ...   \n",
       "2    [[CLS], collateral, damage, finally, delivers,...   \n",
       "3    [[CLS], exciting, and, direct, ,, with, ghost,...   \n",
       "4    [[CLS], and, when, you, ', re, talking, about,...   \n",
       "..                                                 ...   \n",
       "145  [[CLS], it, ', s, a, bit, disappointing, that,...   \n",
       "146  [[CLS], a, bree, ##zy, romantic, comedy, that,...   \n",
       "147  [[CLS], the, film, tries, too, hard, to, be, f...   \n",
       "148  [[CLS], thanks, to, scott, ', s, charismatic, ...   \n",
       "149  [[CLS], drops, you, into, a, dizzy, ##ing, ,, ...   \n",
       "\n",
       "                                             Input IDs  Number of Tokens  \\\n",
       "0    [101, 2012, 2560, 2028, 3496, 2003, 2061, 1942...                20   \n",
       "1    [101, 2130, 1996, 10418, 10026, 6187, 1050, 10...                44   \n",
       "2    [101, 24172, 4053, 2633, 18058, 1996, 5350, 20...                14   \n",
       "3    [101, 10990, 1998, 3622, 1010, 2007, 5745, 134...                20   \n",
       "4    [101, 1998, 2043, 2017, 1005, 2128, 3331, 2055...                22   \n",
       "..                                                 ...               ...   \n",
       "145  [101, 2009, 1005, 1055, 1037, 2978, 15640, 200...                20   \n",
       "146  [101, 1037, 21986, 9096, 6298, 4038, 2008, 203...                24   \n",
       "147  [101, 1996, 2143, 5363, 2205, 2524, 2000, 2022...                18   \n",
       "148  [101, 4283, 2000, 3660, 1005, 1055, 23916, 507...                35   \n",
       "149  [101, 9010, 2017, 2046, 1037, 14849, 2075, 101...                37   \n",
       "\n",
       "                                 Softmax Probs  Predicted Label  True Label  \\\n",
       "0    [0.49043747782707214, 0.5095624923706055]                1           0   \n",
       "1      [0.4584480822086334, 0.541551947593689]                1           0   \n",
       "2     [0.47445327043533325, 0.525546669960022]                1           1   \n",
       "3      [0.443885862827301, 0.5561141967773438]                1           1   \n",
       "4     [0.4730995297431946, 0.5269004702568054]                1           0   \n",
       "..                                         ...              ...         ...   \n",
       "145   [0.4603593051433563, 0.5396407246589661]                1           0   \n",
       "146  [0.46655434370040894, 0.5334456562995911]                1           1   \n",
       "147   [0.4715810716152191, 0.5284189581871033]                1           0   \n",
       "148    [0.4877658486366272, 0.512234091758728]                1           1   \n",
       "149  [0.47137606143951416, 0.5286239385604858]                1           1   \n",
       "\n",
       "    Sentiment  \n",
       "0    positive  \n",
       "1    positive  \n",
       "2    positive  \n",
       "3    positive  \n",
       "4    positive  \n",
       "..        ...  \n",
       "145  positive  \n",
       "146  positive  \n",
       "147  positive  \n",
       "148  positive  \n",
       "149  positive  \n",
       "\n",
       "[150 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_non_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e50d387-fa63-4260-af7e-3a705f2546ec",
   "metadata": {
    "id": "4e50d387-fa63-4260-af7e-3a705f2546ec"
   },
   "source": [
    "## prepare datasets for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5214926c-9aec-4347-8eba-ab678498f8df",
   "metadata": {
    "id": "5214926c-9aec-4347-8eba-ab678498f8df",
    "outputId": "1a7951ae-ee9f-438a-ba02-91c2d937ebd4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'input_ids', 'attention_mask']\n",
      "['label', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Remove the 'sentence' and 'idx' columns from the tokenized_train dataset\n",
    "tokenized_train = tokenized_train.remove_columns([\"sentence\", \"idx\"])\n",
    "\n",
    "# Remove the 'sentence' and 'idx' columns from the tokenized_test dataset\n",
    "tokenized_test = tokenized_test.remove_columns([\"sentence\", \"idx\"])\n",
    "\n",
    "# Check the columns after removal (optional, for confirmation)\n",
    "print(tokenized_train.column_names)\n",
    "print(tokenized_test.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "448991c0-2d04-48c9-a934-1f6a0e82b16f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        Dataset\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 600\n",
       "})\n",
       "\u001b[0;31mLength:\u001b[0m      600\n",
       "\u001b[0;31mFile:\u001b[0m        ~/.conda/envs/default/lib/python3.9/site-packages/datasets/arrow_dataset.py\n",
       "\u001b[0;31mDocstring:\u001b[0m   A Dataset backed by an Arrow table.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79a09813-76bb-40ab-8288-f419a9e73fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25878e34-238b-4347-abce-6903d84887d9",
   "metadata": {
    "id": "25878e34-238b-4347-abce-6903d84887d9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hugging Face's Trainer API expects the data in PyTorch format.\n",
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_test.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "579f7081-f982-41b6-ac63-7db027c5b784",
   "metadata": {
    "id": "579f7081-f982-41b6-ac63-7db027c5b784",
    "outputId": "6bd0b892-f49f-492e-aeaf-d8385c27ebac",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        Dataset\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 600\n",
       "})\n",
       "\u001b[0;31mLength:\u001b[0m      600\n",
       "\u001b[0;31mFile:\u001b[0m        ~/.conda/envs/default/lib/python3.9/site-packages/datasets/arrow_dataset.py\n",
       "\u001b[0;31mDocstring:\u001b[0m   A Dataset backed by an Arrow table.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e8ffa-902a-431b-a35e-25c96aad5923",
   "metadata": {
    "id": "733e8ffa-902a-431b-a35e-25c96aad5923"
   },
   "source": [
    "# Finetuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f6a8ad4-fe2d-4ced-809b-9baa46f5542c",
   "metadata": {
    "id": "8f6a8ad4-fe2d-4ced-809b-9baa46f5542c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cd56f01-cf4a-4716-a992-051aa52e93ee",
   "metadata": {
    "id": "8cd56f01-cf4a-4716-a992-051aa52e93ee",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m      \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           DistilBertForSequenceClassification\n",
       "\u001b[0;31mString form:\u001b[0m   \n",
       "DistilBertForSequenceClassification(\n",
       "           (distilbert): DistilBertModel(\n",
       "           (embeddings): Embedding <...> : Linear(in_features=768, out_features=2, bias=True)\n",
       "           (dropout): Dropout(p=0.2, inplace=False)\n",
       "           )\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.conda/envs/default/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n",
       "pooled output) e.g. for GLUE tasks.\n",
       "\n",
       "\n",
       "This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
       "library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
       "etc.)\n",
       "\n",
       "This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
       "Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
       "and behavior.\n",
       "\n",
       "Parameters:\n",
       "    config ([`DistilBertConfig`]): Model configuration class with all the parameters of the model.\n",
       "        Initializing with a config file does not load the weights associated with the model, only the\n",
       "        configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0deb2a00-44a0-41e1-be14-0a9add0b229a",
   "metadata": {
    "id": "0deb2a00-44a0-41e1-be14-0a9add0b229a",
    "outputId": "a62483f2-7e63-455d-db6e-97466471b267",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.44.2\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc46a7-b9f3-496f-8c1d-7c6700e68edd",
   "metadata": {
    "id": "32cc46a7-b9f3-496f-8c1d-7c6700e68edd",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Detailed Explanation of `DistilBertConfig`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd8331-9a0f-4d49-b453-eb0d2bf23c53",
   "metadata": {
    "id": "49fd8331-9a0f-4d49-b453-eb0d2bf23c53"
   },
   "source": [
    "\n",
    "This configuration describes the architecture and hyperparameters for the `DistilBERT` model. Below is an in-depth explanation of each field in the configuration:\n",
    "\n",
    "1. **`_name_or_path`: \"distilbert-base-uncased\"`  \n",
    "   - This specifies the name or path of the pretrained model.\n",
    "   - `\"distilbert-base-uncased\"` is a smaller, lighter version of BERT that removes the case sensitivity of text (i.e., it treats \"Hello\" and \"hello\" the same way).\n",
    "   \n",
    "2. **`activation`: \"gelu\"`  \n",
    "   - This defines the activation function used in the model.  \n",
    "   - `\"gelu\"` stands for **Gaussian Error Linear Unit**, which is a smoother version of ReLU and commonly used in transformer models.\n",
    "\n",
    "3. **`architectures`: [\"DistilBertForMaskedLM\"]**  \n",
    "   - This indicates the type of architecture being used.  \n",
    "   - `DistilBertForMaskedLM` is the architecture for **Masked Language Modeling**, where the model predicts missing or masked words in sentences. This is used for pretraining BERT-based models.\n",
    "\n",
    "4. **`attention_dropout`: 0.1**  \n",
    "   - Dropout rate for the attention layers.  \n",
    "   - Dropout is a regularization technique used to prevent overfitting by randomly setting a fraction of the attention scores to zero during training. In this case, the rate is 10% (0.1).\n",
    "\n",
    "5. **`dim`: 768**  \n",
    "   - The dimensionality of the hidden representations in the model.  \n",
    "   - Each input token is represented by a vector of size 768 in this version of DistilBERT.\n",
    "\n",
    "6. **`dropout`: 0.1**  \n",
    "   - The general dropout rate applied throughout the model.  \n",
    "   - This helps prevent overfitting by randomly dropping 10% of the neurons during training.\n",
    "\n",
    "7. **`hidden_dim`: 3072**  \n",
    "   - This represents the size of the hidden layer in the feedforward neural network part of the transformer model.  \n",
    "   - Specifically, this is the size of the intermediate layer in each transformer block, which typically has a larger dimension (3072) compared to the input/output dimension (768).\n",
    "8. **`initializer_range`: 0.02**  \n",
    "   - This defines the range used to initialize the weights in the model.  \n",
    "   - The modelâ€™s weights are initialized using a uniform distribution in the range [-0.02, 0.02].\n",
    "\n",
    "9. **`max_position_embeddings`: 512**  \n",
    "   - The maximum number of tokens or positions that the model can handle.  \n",
    "   - For DistilBERT, this is capped at 512 tokens. Any input longer than 512 tokens will be truncated.\n",
    "\n",
    "10. **`model_type`: \"distilbert\"`  \n",
    "   - This defines the type of model being used.  \n",
    "   - `distilbert` is a distilled version of the BERT model, which retains 97% of BERTâ€™s performance but is 60% faster and smaller in size.\n",
    "\n",
    "11. **`n_heads`: 12**  \n",
    "   - The number of attention heads in the multi-head attention mechanism.  \n",
    "   - In transformer architectures like BERT, the attention mechanism is split into multiple \"heads\" that focus on different parts of the input sequence. DistilBERT uses 12 attention heads.\n",
    "\n",
    "12. **`n_layers`: 6**  \n",
    "   - The number of layers (transformer blocks) in the model.  \n",
    "   - DistilBERT has 6 layers, as opposed to the 12 layers in BERT. This reduction is one reason why DistilBERT is faster and smaller.\n",
    "\n",
    "13. **`pad_token_id`: 0**  \n",
    "   - The token ID used to represent padding in the input sequence.  \n",
    "   - Padding tokens are added to make all sequences in a batch the same length, and `0` is the ID for the padding token.\n",
    "\n",
    "14. **`qa_dropout`: 0.1**  \n",
    "   - Dropout rate applied during the Question Answering (QA) head of the model.  \n",
    "   - This is used in tasks like SQuAD (Stanford Question Answering Dataset), where a 10% dropout rate is applied.\n",
    "\n",
    "15. **`seq_classif_dropout`: 0.2**  \n",
    "   - Dropout rate used in the sequence classification head of the model.  \n",
    "   - This is applicable for tasks like text classification, where a 20% dropout rate is applied to prevent overfitting.\n",
    "\n",
    "16. **`sinusoidal_pos_embds`: false**  \n",
    "   - This flag indicates whether sinusoidal positional embeddings are used.  \n",
    "   - DistilBERT uses learned positional embeddings (as in the original BERT) instead of sinusoidal ones.\n",
    "\n",
    "17. **`tie_weights_`: true**  \n",
    "   - This indicates whether the weights of the embeddings and the output layer are tied.  \n",
    "   - Weight tying reduces the number of parameters in the model and ensures that the input and output embeddings are similar.\n",
    "\n",
    "18. **`transformers_version`: \"4.44.0\"**  \n",
    "   - This specifies the version of the Hugging Face Transformers library used to configure the model.  \n",
    "   - In this case, the version is 4.44.0.\n",
    "\n",
    "19. **`vocab_size`: 30522**  \n",
    "   - The size of the vocabulary used by the tokenizer and the model.  \n",
    "   - DistilBERT inherits the BERT tokenizer, which uses a vocabulary of 30,522 tokens. This includes words, subwords, and special tokens (like [PAD], [CLS], etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9bbb9e-d86d-4799-86eb-a3bb2e95627d",
   "metadata": {
    "id": "ed9bbb9e-d86d-4799-86eb-a3bb2e95627d"
   },
   "source": [
    "## training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9d2679d-08aa-4fab-b845-e777b3b8a632",
   "metadata": {
    "id": "f9d2679d-08aa-4fab-b845-e777b3b8a632",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load accuracy as the evaluation metric. This will be used to compute\n",
    "# the accuracy of the model on the validation dataset during evaluation.\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define the function to compute metrics (accuracy in this case).\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d256a14-d35d-4d11-b102-fa3fe76b6732",
   "metadata": {
    "id": "6d256a14-d35d-4d11-b102-fa3fe76b6732",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the training arguments, which control how the model will be trained.\n",
    "# Each argument has a direct or indirect impact on both the computation time\n",
    "# and the model's final performance.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Directory where the model's checkpoints\n",
    "                                     # and outputs will be saved.\n",
    "                                     # (Doesn't directly affect training time)\n",
    "\n",
    "    eval_steps=5,                    # Evaluate the model every  eval_steps.\n",
    "                                     # Frequent evaluations can slow down training,\n",
    "                                     # but provide insights into model performance\n",
    "                                     # during training\n",
    "\n",
    "    learning_rate=2e-5,              # Learning rate controls the speed at which\n",
    "                                     # the model updates weights during training.\n",
    "                                     # A higher rate may lead to faster convergence,\n",
    "                                     # but could also risk overshooting optima,\n",
    "                                     # while a lower rate results in slower but\n",
    "                                     # potentially more stable training.\n",
    "\n",
    "    per_device_train_batch_size=16,  # Batch size for training on each device (GPU/CPU).\n",
    "                                     # A larger batch size speeds up training by\n",
    "                                     # processing more data per step, but uses more memory.\n",
    "                                     # If you run out of memory, reduce this value.\n",
    "                                     # Smaller batch sizes mean more updates per epoch.\n",
    "\n",
    "    per_device_eval_batch_size=64,   # Batch size for evaluation (validation/test set).\n",
    "                                     # Larger batch sizes can make evaluation faster\n",
    "                                     # but require more memory. Evaluation only happens\n",
    "                                     # during the validation phase, so it doesn't affect\n",
    "                                     # the training speed.\n",
    "\n",
    "    num_train_epochs=4,              # Number of training epochs. Each epoch is one full\n",
    "                                     # pass through the training dataset. More epochs\n",
    "                                     # increase training time but give the model more\n",
    "                                     # chances to learn. Fewer epochs result in faster\n",
    "                                     # training but risk underfitting the model.\n",
    "\n",
    "    gradient_accumulation_steps=3,   # Accumulate gradients over multiple steps before\n",
    "                                     # updating model weights. This simulates a larger\n",
    "                                     # batch size (e.g., with batch_size=16 and\n",
    "                                     # gradient_accumulation_steps=3, the model behaves\n",
    "                                     # like batch_size=48). This reduces memory usage\n",
    "                                     # but slows down training because updates happen\n",
    "                                     # less frequently.\n",
    "\n",
    "    weight_decay=0.01,               # Weight decay applies regularization during training\n",
    "                                     # to prevent overfitting by penalizing large weights.\n",
    "                                     # It improves generalization and helps ensure that\n",
    "                                     # the model performs well on unseen data.\n",
    "\n",
    "    logging_dir=\"./logs\",            # Directory for saving logs. Logging doesn't directly\n",
    "                                     # affect training speed, but frequent logging\n",
    "                                     # (e.g., at every step) can slow down the process.\n",
    "                                     # Set appropriate intervals for logging to balance\n",
    "                                     # information and speed.\n",
    "\n",
    "    logging_steps=100,               # Log metrics every 100 steps. Too frequent logging\n",
    "                                     # can slow training down, while infrequent logging\n",
    "                                     # might not provide enough insight into the model's\n",
    "                                     # performance during training. Adjust based on your\n",
    "                                     # need for monitoring.\n",
    "\n",
    "    save_strategy=\"epoch\",           # Save the model's checkpoints at the end of each\n",
    "                                     # epoch. This is generally efficient and safe\n",
    "                                     # unless you need more frequent saving (e.g., \"steps\").\n",
    "                                     # More frequent saving can slow down training,\n",
    "                                     # as saving checkpoints takes time.\n",
    "\n",
    "    load_best_model_at_end=True,     # Load the best model based on the evaluation\n",
    "                                     # metric after training finishes. While this\n",
    "                                     # doesn't affect training speed, it ensures the\n",
    "                                     # best-performing model (usually evaluated on\n",
    "                                     # validation accuracy or loss) is kept.\n",
    "\n",
    "    metric_for_best_model=\"accuracy\",# Monitor accuracy to select the best model.\n",
    "                                     # This defines the metric used to determine\n",
    "                                     # which model is considered the best when\n",
    "                                     # `load_best_model_at_end` is set to True.\n",
    "\n",
    "    evaluation_strategy=\"epoch\",     # Run evaluation at the end of each epoch.\n",
    "                                     # This balances training and evaluation time,\n",
    "                                     # allowing for regular checks on validation\n",
    "                                     # performance without frequent interruptions.\n",
    "\n",
    "    report_to=\"none\",                # No need to report results to external platforms\n",
    "                                     # like TensorBoard or Weights & Biases. This keeps\n",
    "                                     # overhead minimal and speeds up the training process\n",
    "                                     # if you're not interested in reporting metrics\n",
    "                                     # elsewhere.\n",
    "\n",
    "    seed=42                          # Sets a fixed random seed to ensure reproducibility.\n",
    "                                     # Doesn't affect computation time but helps ensure\n",
    "                                     # the same results on re-runs.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6125ca4c-4b61-4225-9d45-da303f5fc0ab",
   "metadata": {
    "id": "6125ca4c-4b61-4225-9d45-da303f5fc0ab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                     # The model to fine-tune\n",
    "    args=training_args,              # Training arguments (from TrainingArguments)\n",
    "    train_dataset=tokenized_train,   # The tokenized training dataset\n",
    "    eval_dataset=tokenized_test,     # The tokenized evaluation dataset\n",
    "    compute_metrics=compute_metrics, # Function to compute evaluation metrics\n",
    "\n",
    "    # Additional Arguments\n",
    "    tokenizer=tokenizer,             # The tokenizer to use (optional, but useful if you\n",
    "                                     # want to use it for decoding or processing inputs).\n",
    "\n",
    "    data_collator=None,              # A function to prepare batches of data. This is\n",
    "                                     # typically left as `None`, and the default\n",
    "                                     # collator is used, but you can define your own\n",
    "                                     # data collator if necessary (e.g., for dynamic\n",
    "                                     # padding).\n",
    "\n",
    "    optimizers=(None, None),         # You can provide your own optimizer and scheduler\n",
    "                                     # (learning rate scheduler). If `None`, the default\n",
    "                                     # AdamW optimizer and linear scheduler are used.\n",
    "\n",
    "    callbacks=None,                  # List of callbacks, such as `EarlyStoppingCallback`,\n",
    "                                     # to run during training. Callbacks can be used\n",
    "                                     # to perform additional actions during training.\n",
    "\n",
    "    preprocess_logits_for_metrics=None,  # If you want to pre-process logits before\n",
    "                                         # computing metrics, define a function here.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22028822-7885-42d8-9474-e20a6e26ce4c",
   "metadata": {
    "id": "22028822-7885-42d8-9474-e20a6e26ce4c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#trainer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "585c5804-3cf4-4d7d-990d-e2f0ce2eebd7",
   "metadata": {
    "id": "585c5804-3cf4-4d7d-990d-e2f0ce2eebd7",
    "outputId": "d55f182b-66b7-451a-cf54-b2a38e2ebf90",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 05:18, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.675232</td>\n",
       "      <td>0.506667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.598589</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.489973</td>\n",
       "      <td>0.873333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=48, training_loss=0.5745113690694174, metrics={'train_runtime': 326.155, 'train_samples_per_second': 7.358, 'train_steps_per_second': 0.147, 'total_flos': 31264375885920.0, 'train_loss': 0.5745113690694174, 'epoch': 3.7894736842105265})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "526bf3b2-a324-4a02-aa82-6726d3649ac5",
   "metadata": {
    "id": "526bf3b2-a324-4a02-aa82-6726d3649ac5",
    "outputId": "48c2b14f-7a45-4424-e394-3d336bfd0dcb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run evaluation on both training and validation datasets\n",
    "train_results = trainer.evaluate(eval_dataset=tokenized_train)  # Evaluate on training set\n",
    "test_results = trainer.evaluate(eval_dataset=tokenized_test)    # Evaluate on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e38d4b0d-6f2a-4de8-a13f-4fa7ffa740f6",
   "metadata": {
    "id": "e38d4b0d-6f2a-4de8-a13f-4fa7ffa740f6",
    "outputId": "cb495624-5487-4fb2-fb25-74fe9d52e598",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4245406687259674,\n",
       " 'eval_accuracy': 0.8816666666666667,\n",
       " 'eval_runtime': 19.1689,\n",
       " 'eval_samples_per_second': 31.301,\n",
       " 'eval_steps_per_second': 0.522,\n",
       " 'epoch': 3.7894736842105265}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72714d56-c6c2-45dc-ab45-673ea1f88064",
   "metadata": {
    "id": "72714d56-c6c2-45dc-ab45-673ea1f88064",
    "outputId": "b3c14914-510b-4a60-8c54-0953bd9393ba",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.48997294902801514,\n",
       " 'eval_accuracy': 0.8733333333333333,\n",
       " 'eval_runtime': 4.4629,\n",
       " 'eval_samples_per_second': 33.611,\n",
       " 'eval_steps_per_second': 0.672,\n",
       " 'epoch': 3.7894736842105265}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b522bd-0077-499a-9993-668fbb1d5df7",
   "metadata": {
    "id": "82b522bd-0077-499a-9993-668fbb1d5df7"
   },
   "source": [
    "## labelling with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4210f56d-569a-445f-a74e-f23f777603bf",
   "metadata": {
    "id": "4210f56d-569a-445f-a74e-f23f777603bf",
    "outputId": "04448d5c-24a9-4838-f454-338b4e9f6c55"
   },
   "outputs": [],
   "source": [
    "sentence_label = predict_label_sentence(\"I am happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5b396d6-7161-4352-ad18-e4e379e9e281",
   "metadata": {
    "id": "b5b396d6-7161-4352-ad18-e4e379e9e281"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_sentence': 'I am happy',\n",
       " 'tokenized_sentence': ['[CLS]', 'i', 'am', 'happy', '[SEP]'],\n",
       " 'input_ids': [101, 1045, 2572, 3407, 102],\n",
       " 'softmax_probs': [0.19336703419685364, 0.8066329956054688],\n",
       " 'predicted_label': 1,\n",
       " 'sentiment': 'positive'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c599ffd8-8c6f-4870-851e-2329dbe5a3ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labelling Sentences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:10<00:00, 14.60it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test_trained, accuracy_test_trained = predict_label_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1bba9e97-18c0-47f0-8338-140b5fb201d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tokenized Sentence</th>\n",
       "      <th>Input IDs</th>\n",
       "      <th>Number of Tokens</th>\n",
       "      <th>Softmax Probs</th>\n",
       "      <th>Predicted Label</th>\n",
       "      <th>True Label</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>at least one scene is so disgusting that viewe...</td>\n",
       "      <td>[[CLS], at, least, one, scene, is, so, disgust...</td>\n",
       "      <td>[101, 2012, 2560, 2028, 3496, 2003, 2061, 1942...</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.6496649980545044, 0.350335031747818]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>even the finest chef ca n't make a hotdog into...</td>\n",
       "      <td>[[CLS], even, the, finest, chef, ca, n, ', t, ...</td>\n",
       "      <td>[101, 2130, 1996, 10418, 10026, 6187, 1050, 10...</td>\n",
       "      <td>44</td>\n",
       "      <td>[0.564052939414978, 0.4359470307826996]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>collateral damage finally delivers the goods f...</td>\n",
       "      <td>[[CLS], collateral, damage, finally, delivers,...</td>\n",
       "      <td>[101, 24172, 4053, 2633, 18058, 1996, 5350, 20...</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.5163972973823547, 0.48360273241996765]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exciting and direct , with ghost imagery that ...</td>\n",
       "      <td>[[CLS], exciting, and, direct, ,, with, ghost,...</td>\n",
       "      <td>[101, 10990, 1998, 3622, 1010, 2007, 5745, 134...</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.1598498672246933, 0.8401501178741455]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and when you 're talking about a slapstick com...</td>\n",
       "      <td>[[CLS], and, when, you, ', re, talking, about,...</td>\n",
       "      <td>[101, 1998, 2043, 2017, 1005, 2128, 3331, 2055...</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.6049067974090576, 0.39509323239326477]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>it 's a bit disappointing that it only manages...</td>\n",
       "      <td>[[CLS], it, ', s, a, bit, disappointing, that,...</td>\n",
       "      <td>[101, 2009, 1005, 1055, 1037, 2978, 15640, 200...</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.5640332102775574, 0.4359667897224426]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>a breezy romantic comedy that has the punch of...</td>\n",
       "      <td>[[CLS], a, bree, ##zy, romantic, comedy, that,...</td>\n",
       "      <td>[101, 1037, 21986, 9096, 6298, 4038, 2008, 203...</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.18238702416419983, 0.8176130056381226]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>the film tries too hard to be funny and tries ...</td>\n",
       "      <td>[[CLS], the, film, tries, too, hard, to, be, f...</td>\n",
       "      <td>[101, 1996, 2143, 5363, 2205, 2524, 2000, 2022...</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.614990770816803, 0.3850092589855194]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>thanks to scott 's charismatic roger and eisen...</td>\n",
       "      <td>[[CLS], thanks, to, scott, ', s, charismatic, ...</td>\n",
       "      <td>[101, 4283, 2000, 3660, 1005, 1055, 23916, 507...</td>\n",
       "      <td>35</td>\n",
       "      <td>[0.2577976882457733, 0.7422022819519043]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>drops you into a dizzying , volatile , pressur...</td>\n",
       "      <td>[[CLS], drops, you, into, a, dizzy, ##ing, ,, ...</td>\n",
       "      <td>[101, 9010, 2017, 2046, 1037, 14849, 2075, 101...</td>\n",
       "      <td>37</td>\n",
       "      <td>[0.6184784173965454, 0.381521612405777]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  \\\n",
       "0    at least one scene is so disgusting that viewe...   \n",
       "1    even the finest chef ca n't make a hotdog into...   \n",
       "2    collateral damage finally delivers the goods f...   \n",
       "3    exciting and direct , with ghost imagery that ...   \n",
       "4    and when you 're talking about a slapstick com...   \n",
       "..                                                 ...   \n",
       "145  it 's a bit disappointing that it only manages...   \n",
       "146  a breezy romantic comedy that has the punch of...   \n",
       "147  the film tries too hard to be funny and tries ...   \n",
       "148  thanks to scott 's charismatic roger and eisen...   \n",
       "149  drops you into a dizzying , volatile , pressur...   \n",
       "\n",
       "                                    Tokenized Sentence  \\\n",
       "0    [[CLS], at, least, one, scene, is, so, disgust...   \n",
       "1    [[CLS], even, the, finest, chef, ca, n, ', t, ...   \n",
       "2    [[CLS], collateral, damage, finally, delivers,...   \n",
       "3    [[CLS], exciting, and, direct, ,, with, ghost,...   \n",
       "4    [[CLS], and, when, you, ', re, talking, about,...   \n",
       "..                                                 ...   \n",
       "145  [[CLS], it, ', s, a, bit, disappointing, that,...   \n",
       "146  [[CLS], a, bree, ##zy, romantic, comedy, that,...   \n",
       "147  [[CLS], the, film, tries, too, hard, to, be, f...   \n",
       "148  [[CLS], thanks, to, scott, ', s, charismatic, ...   \n",
       "149  [[CLS], drops, you, into, a, dizzy, ##ing, ,, ...   \n",
       "\n",
       "                                             Input IDs  Number of Tokens  \\\n",
       "0    [101, 2012, 2560, 2028, 3496, 2003, 2061, 1942...                20   \n",
       "1    [101, 2130, 1996, 10418, 10026, 6187, 1050, 10...                44   \n",
       "2    [101, 24172, 4053, 2633, 18058, 1996, 5350, 20...                14   \n",
       "3    [101, 10990, 1998, 3622, 1010, 2007, 5745, 134...                20   \n",
       "4    [101, 1998, 2043, 2017, 1005, 2128, 3331, 2055...                22   \n",
       "..                                                 ...               ...   \n",
       "145  [101, 2009, 1005, 1055, 1037, 2978, 15640, 200...                20   \n",
       "146  [101, 1037, 21986, 9096, 6298, 4038, 2008, 203...                24   \n",
       "147  [101, 1996, 2143, 5363, 2205, 2524, 2000, 2022...                18   \n",
       "148  [101, 4283, 2000, 3660, 1005, 1055, 23916, 507...                35   \n",
       "149  [101, 9010, 2017, 2046, 1037, 14849, 2075, 101...                37   \n",
       "\n",
       "                                 Softmax Probs  Predicted Label  True Label  \\\n",
       "0      [0.6496649980545044, 0.350335031747818]                0           0   \n",
       "1      [0.564052939414978, 0.4359470307826996]                0           0   \n",
       "2    [0.5163972973823547, 0.48360273241996765]                0           1   \n",
       "3     [0.1598498672246933, 0.8401501178741455]                1           1   \n",
       "4    [0.6049067974090576, 0.39509323239326477]                0           0   \n",
       "..                                         ...              ...         ...   \n",
       "145   [0.5640332102775574, 0.4359667897224426]                0           0   \n",
       "146  [0.18238702416419983, 0.8176130056381226]                1           1   \n",
       "147    [0.614990770816803, 0.3850092589855194]                0           0   \n",
       "148   [0.2577976882457733, 0.7422022819519043]                1           1   \n",
       "149    [0.6184784173965454, 0.381521612405777]                0           1   \n",
       "\n",
       "    Sentiment  \n",
       "0    negative  \n",
       "1    negative  \n",
       "2    negative  \n",
       "3    positive  \n",
       "4    negative  \n",
       "..        ...  \n",
       "145  negative  \n",
       "146  positive  \n",
       "147  negative  \n",
       "148  positive  \n",
       "149  negative  \n",
       "\n",
       "[150 rows x 8 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "285f61e8-545b-42c2-bccf-ec961bcf7435",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8733333333333333"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_test_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951f027b-f4bc-46e7-bf49-21fbf1bc86cd",
   "metadata": {
    "id": "951f027b-f4bc-46e7-bf49-21fbf1bc86cd"
   },
   "source": [
    "# compatible datasasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37b6a6d6-6aa3-425b-b8d3-ebb10e6a7071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of compatible datasets\n",
    "compatible_datasets = {\n",
    "    \"imdb\": \"imdb\",\n",
    "    \"yelp\": \"yelp_polarity\",\n",
    "    \"amazon\": \"amazon_polarity\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8a3e5d1-3583-422d-b66e-16dd6b9f38a3",
   "metadata": {
    "id": "e8a3e5d1-3583-422d-b66e-16dd6b9f38a3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_select_samples(dataset_name, n_samples):\n",
    "    \"\"\"\n",
    "    Downloads a dataset for sentiment analysis and selects a random subset of n_samples.\n",
    "\n",
    "    Args:\n",
    "    - dataset_name: Name of the dataset (must be one of the compatible datasets).\n",
    "    - n_samples: Number of random samples to select.\n",
    "\n",
    "    Returns:\n",
    "    - new_data: A subset of the dataset with n_samples randomly selected.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Load the dataset\n",
    "    if dataset_name not in compatible_datasets:\n",
    "        raise ValueError(f\"Dataset '{dataset_name}' not found. Choose from {list(compatible_datasets.keys())}\")\n",
    "\n",
    "    dataset_info = compatible_datasets[dataset_name]\n",
    "\n",
    "    # Some datasets require specifying a subset\n",
    "    if isinstance(dataset_info, tuple):\n",
    "        dataset = load_dataset(*dataset_info)\n",
    "    else:\n",
    "        dataset = load_dataset(dataset_info)\n",
    "\n",
    "    # Use the test split if available, otherwise use the train split\n",
    "    split = 'test' if 'test' in dataset else 'train'\n",
    "    data = dataset[split]\n",
    "\n",
    "    # Step 2: Select a random sample of n_samples from the dataset\n",
    "    new_data = data.shuffle(seed=17).select(range(n_samples))\n",
    "\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9da99e-ba90-43c7-8a45-4387afac1af6",
   "metadata": {
    "id": "2e9da99e-ba90-43c7-8a45-4387afac1af6"
   },
   "source": [
    "### imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69c10bba-99fa-453d-a131-3f709895418f",
   "metadata": {
    "id": "69c10bba-99fa-453d-a131-3f709895418f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02e606b9f534a65afa2c040a8467207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_data = download_and_select_samples(dataset_name = \"imdb\", n_samples = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90211fc8-7a71-4d78-b088-0179b0fd09ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 150\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "edf2c8fd-9e1c-4158-ab80-d1aa690e18cd",
   "metadata": {
    "id": "edf2c8fd-9e1c-4158-ab80-d1aa690e18cd",
    "outputId": "33d94e8a-74c9-4f8c-a453-378a09c9b59a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labelling Sentences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:38<00:00,  3.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the selected new_data\n",
    "imdb_df, imdb_accuracy = predict_label_dataset(imdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b58018d-2549-4e65-8e58-79f066828e18",
   "metadata": {
    "id": "6b58018d-2549-4e65-8e58-79f066828e18",
    "outputId": "30d0f493-9f7c-427c-9b0c-20a9eb8126ff",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy imdb: 0.8533333333333334\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy imdb: {imdb_accuracy}\")\n",
    "#print(imdb_df.head())  # Display the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae4cd265-4e42-48a5-8375-fe9ed0435452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tokenized Sentence</th>\n",
       "      <th>Input IDs</th>\n",
       "      <th>Number of Tokens</th>\n",
       "      <th>Softmax Probs</th>\n",
       "      <th>Predicted Label</th>\n",
       "      <th>True Label</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I wish I had read the comments on IMDb before ...</td>\n",
       "      <td>[[CLS], i, wish, i, had, read, the, comments, ...</td>\n",
       "      <td>[101, 1045, 4299, 1045, 2018, 3191, 1996, 7928...</td>\n",
       "      <td>159</td>\n",
       "      <td>[0.5610942244529724, 0.4389057457447052]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I loved this movie! So worth the long running ...</td>\n",
       "      <td>[[CLS], i, loved, this, movie, !, so, worth, t...</td>\n",
       "      <td>[101, 1045, 3866, 2023, 3185, 999, 2061, 4276,...</td>\n",
       "      <td>149</td>\n",
       "      <td>[0.4507291316986084, 0.5492709279060364]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I actually went to see this movie with low exp...</td>\n",
       "      <td>[[CLS], i, actually, went, to, see, this, movi...</td>\n",
       "      <td>[101, 1045, 2941, 2253, 2000, 2156, 2023, 3185...</td>\n",
       "      <td>222</td>\n",
       "      <td>[0.39913123846054077, 0.6008687615394592]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For anyone who cares to know something about t...</td>\n",
       "      <td>[[CLS], for, anyone, who, cares, to, know, som...</td>\n",
       "      <td>[101, 2005, 3087, 2040, 14977, 2000, 2113, 224...</td>\n",
       "      <td>201</td>\n",
       "      <td>[0.522199273109436, 0.4778006672859192]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eric Idle, Robbie Coltraine, Janet Suzman - it...</td>\n",
       "      <td>[[CLS], eric, idle, ,, robbie, colt, ##raine, ...</td>\n",
       "      <td>[101, 4388, 18373, 1010, 12289, 9110, 26456, 1...</td>\n",
       "      <td>125</td>\n",
       "      <td>[0.5400406122207642, 0.45995938777923584]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>I bought this Chuck Norris DVD knowing that it...</td>\n",
       "      <td>[[CLS], i, bought, this, chuck, norris, dvd, k...</td>\n",
       "      <td>[101, 1045, 4149, 2023, 8057, 15466, 4966, 420...</td>\n",
       "      <td>178</td>\n",
       "      <td>[0.4604353904724121, 0.5395646095275879]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>This movie is based on the book, \"A Many Splen...</td>\n",
       "      <td>[[CLS], this, movie, is, based, on, the, book,...</td>\n",
       "      <td>[101, 2023, 3185, 2003, 2241, 2006, 1996, 2338...</td>\n",
       "      <td>266</td>\n",
       "      <td>[0.31596124172210693, 0.6840387582778931]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>I must say, when I saw this film at a 6.5 on t...</td>\n",
       "      <td>[[CLS], i, must, say, ,, when, i, saw, this, f...</td>\n",
       "      <td>[101, 1045, 2442, 2360, 1010, 2043, 1045, 2387...</td>\n",
       "      <td>500</td>\n",
       "      <td>[0.5218430757522583, 0.4781569540500641]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>I really enjoyed this movie. I have a real sen...</td>\n",
       "      <td>[[CLS], i, really, enjoyed, this, movie, ., i,...</td>\n",
       "      <td>[101, 1045, 2428, 5632, 2023, 3185, 1012, 1045...</td>\n",
       "      <td>350</td>\n",
       "      <td>[0.4625762701034546, 0.5374237298965454]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>...and boy is the collision deafening. A femal...</td>\n",
       "      <td>[[CLS], ., ., ., and, boy, is, the, collision,...</td>\n",
       "      <td>[101, 1012, 1012, 1012, 1998, 2879, 2003, 1996...</td>\n",
       "      <td>148</td>\n",
       "      <td>[0.5410314798355103, 0.45896854996681213]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  \\\n",
       "0    I wish I had read the comments on IMDb before ...   \n",
       "1    I loved this movie! So worth the long running ...   \n",
       "2    I actually went to see this movie with low exp...   \n",
       "3    For anyone who cares to know something about t...   \n",
       "4    Eric Idle, Robbie Coltraine, Janet Suzman - it...   \n",
       "..                                                 ...   \n",
       "145  I bought this Chuck Norris DVD knowing that it...   \n",
       "146  This movie is based on the book, \"A Many Splen...   \n",
       "147  I must say, when I saw this film at a 6.5 on t...   \n",
       "148  I really enjoyed this movie. I have a real sen...   \n",
       "149  ...and boy is the collision deafening. A femal...   \n",
       "\n",
       "                                    Tokenized Sentence  \\\n",
       "0    [[CLS], i, wish, i, had, read, the, comments, ...   \n",
       "1    [[CLS], i, loved, this, movie, !, so, worth, t...   \n",
       "2    [[CLS], i, actually, went, to, see, this, movi...   \n",
       "3    [[CLS], for, anyone, who, cares, to, know, som...   \n",
       "4    [[CLS], eric, idle, ,, robbie, colt, ##raine, ...   \n",
       "..                                                 ...   \n",
       "145  [[CLS], i, bought, this, chuck, norris, dvd, k...   \n",
       "146  [[CLS], this, movie, is, based, on, the, book,...   \n",
       "147  [[CLS], i, must, say, ,, when, i, saw, this, f...   \n",
       "148  [[CLS], i, really, enjoyed, this, movie, ., i,...   \n",
       "149  [[CLS], ., ., ., and, boy, is, the, collision,...   \n",
       "\n",
       "                                             Input IDs  Number of Tokens  \\\n",
       "0    [101, 1045, 4299, 1045, 2018, 3191, 1996, 7928...               159   \n",
       "1    [101, 1045, 3866, 2023, 3185, 999, 2061, 4276,...               149   \n",
       "2    [101, 1045, 2941, 2253, 2000, 2156, 2023, 3185...               222   \n",
       "3    [101, 2005, 3087, 2040, 14977, 2000, 2113, 224...               201   \n",
       "4    [101, 4388, 18373, 1010, 12289, 9110, 26456, 1...               125   \n",
       "..                                                 ...               ...   \n",
       "145  [101, 1045, 4149, 2023, 8057, 15466, 4966, 420...               178   \n",
       "146  [101, 2023, 3185, 2003, 2241, 2006, 1996, 2338...               266   \n",
       "147  [101, 1045, 2442, 2360, 1010, 2043, 1045, 2387...               500   \n",
       "148  [101, 1045, 2428, 5632, 2023, 3185, 1012, 1045...               350   \n",
       "149  [101, 1012, 1012, 1012, 1998, 2879, 2003, 1996...               148   \n",
       "\n",
       "                                 Softmax Probs  Predicted Label  True Label  \\\n",
       "0     [0.5610942244529724, 0.4389057457447052]                0           0   \n",
       "1     [0.4507291316986084, 0.5492709279060364]                1           1   \n",
       "2    [0.39913123846054077, 0.6008687615394592]                1           1   \n",
       "3      [0.522199273109436, 0.4778006672859192]                0           0   \n",
       "4    [0.5400406122207642, 0.45995938777923584]                0           0   \n",
       "..                                         ...              ...         ...   \n",
       "145   [0.4604353904724121, 0.5395646095275879]                1           0   \n",
       "146  [0.31596124172210693, 0.6840387582778931]                1           1   \n",
       "147   [0.5218430757522583, 0.4781569540500641]                0           0   \n",
       "148   [0.4625762701034546, 0.5374237298965454]                1           1   \n",
       "149  [0.5410314798355103, 0.45896854996681213]                0           0   \n",
       "\n",
       "    Sentiment  \n",
       "0    negative  \n",
       "1    positive  \n",
       "2    positive  \n",
       "3    negative  \n",
       "4    negative  \n",
       "..        ...  \n",
       "145  positive  \n",
       "146  positive  \n",
       "147  negative  \n",
       "148  positive  \n",
       "149  negative  \n",
       "\n",
       "[150 rows x 8 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd73c530-8aa2-43f1-a6a2-097d344a4d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I wish I had read the comments on IMDb before I saw this movie. The first 1 hour was OK, though it did make me wonder why everything was centered at Chicago and why no one reported any weather anomaly from outside US. Isolated acts of nature (of this magnitude) are unthinkable. But beyond the first 60 minutes, the movie just drags on like a never-ending story. The screenplay is horrible. As for the actors, very poor choice. Only the people hired to run in panic stick to their roles. But I do have to agree that this movie has got some good 'special effects'. If you rented it on a DVD and would want to watch the movie, despite the reviews, then play it on maximum speed your player would allow!\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df['Sentence'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8538fa-9a32-4f5b-8730-885494cfc249",
   "metadata": {
    "id": "7e8538fa-9a32-4f5b-8730-885494cfc249"
   },
   "source": [
    "### yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "491887d2-7dc5-4612-91e0-bb8ef8baac84",
   "metadata": {
    "id": "491887d2-7dc5-4612-91e0-bb8ef8baac84",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979f00b7068745d5a02aa1bcdd550ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yelp_data = download_and_select_samples(dataset_name = \"yelp\", n_samples = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2631313a-b454-4084-a28f-9f68ad54cd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 150\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "62da23b4-b60b-4b2d-9379-73e3d08cf9fb",
   "metadata": {
    "id": "62da23b4-b60b-4b2d-9379-73e3d08cf9fb",
    "outputId": "de269d51-6b31-4463-b005-33ddfafc291b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labelling Sentences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:22<00:00,  6.65it/s]\n"
     ]
    }
   ],
   "source": [
    "yelp_df, yelp_accuracy= predict_label_dataset(yelp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "905aaddf-3ff6-44ab-be0e-1c59869da50f",
   "metadata": {
    "id": "905aaddf-3ff6-44ab-be0e-1c59869da50f",
    "outputId": "c3332cb9-11ae-41bd-b569-92323f7d22da",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9266666666666666"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61ab1813-4ca9-44de-aa1d-c13507501eb0",
   "metadata": {
    "id": "61ab1813-4ca9-44de-aa1d-c13507501eb0",
    "outputId": "75631de0-e641-483b-d061-de5152c40a59",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tokenized Sentence</th>\n",
       "      <th>Input IDs</th>\n",
       "      <th>Number of Tokens</th>\n",
       "      <th>Softmax Probs</th>\n",
       "      <th>Predicted Label</th>\n",
       "      <th>True Label</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Service and food were awesome! Highly recommen...</td>\n",
       "      <td>[[CLS], service, and, food, were, awesome, !, ...</td>\n",
       "      <td>[101, 2326, 1998, 2833, 2020, 12476, 999, 3811...</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2670975625514984, 0.732902467250824]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The food was OK, it was kind of slow so the fi...</td>\n",
       "      <td>[[CLS], the, food, was, ok, ,, it, was, kind, ...</td>\n",
       "      <td>[101, 1996, 2833, 2001, 7929, 1010, 2009, 2001...</td>\n",
       "      <td>163</td>\n",
       "      <td>[0.5392808318138123, 0.46071913838386536]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The gym is dirty and old and the whole place i...</td>\n",
       "      <td>[[CLS], the, gym, is, dirty, and, old, and, th...</td>\n",
       "      <td>[101, 1996, 9726, 2003, 6530, 1998, 2214, 1998...</td>\n",
       "      <td>279</td>\n",
       "      <td>[0.5794830918312073, 0.4205169379711151]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just arrived from the overnight train, arrived...</td>\n",
       "      <td>[[CLS], just, arrived, from, the, overnight, t...</td>\n",
       "      <td>[101, 2074, 3369, 2013, 1996, 11585, 3345, 101...</td>\n",
       "      <td>68</td>\n",
       "      <td>[0.275282621383667, 0.724717378616333]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So just in case this is the first review you'v...</td>\n",
       "      <td>[[CLS], so, just, in, case, this, is, the, fir...</td>\n",
       "      <td>[101, 2061, 2074, 1999, 2553, 2023, 2003, 1996...</td>\n",
       "      <td>309</td>\n",
       "      <td>[0.4868742823600769, 0.5131257176399231]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Usually I am not a big stickler for customer s...</td>\n",
       "      <td>[[CLS], usually, i, am, not, a, big, stick, ##...</td>\n",
       "      <td>[101, 2788, 1045, 2572, 2025, 1037, 2502, 6293...</td>\n",
       "      <td>512</td>\n",
       "      <td>[0.4426237642765045, 0.5573763251304626]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Of the cheaper casinos on the Strip, Bally's h...</td>\n",
       "      <td>[[CLS], of, the, cheaper, casinos, on, the, st...</td>\n",
       "      <td>[101, 1997, 1996, 16269, 27300, 2006, 1996, 61...</td>\n",
       "      <td>185</td>\n",
       "      <td>[0.44811585545539856, 0.551884114742279]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Extradinarilly big for a cafe! They've got eve...</td>\n",
       "      <td>[[CLS], extra, ##dina, ##rill, ##y, big, for, ...</td>\n",
       "      <td>[101, 4469, 18979, 24714, 2100, 2502, 2005, 10...</td>\n",
       "      <td>140</td>\n",
       "      <td>[0.4032253921031952, 0.5967746376991272]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>The serving is good, but the steak dinner is n...</td>\n",
       "      <td>[[CLS], the, serving, is, good, ,, but, the, s...</td>\n",
       "      <td>[101, 1996, 3529, 2003, 2204, 1010, 2021, 1996...</td>\n",
       "      <td>32</td>\n",
       "      <td>[0.5420543551445007, 0.45794567465782166]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>We have been here a few times and the food is ...</td>\n",
       "      <td>[[CLS], we, have, been, here, a, few, times, a...</td>\n",
       "      <td>[101, 2057, 2031, 2042, 2182, 1037, 2261, 2335...</td>\n",
       "      <td>142</td>\n",
       "      <td>[0.5248243808746338, 0.4751756191253662]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  \\\n",
       "0    Service and food were awesome! Highly recommen...   \n",
       "1    The food was OK, it was kind of slow so the fi...   \n",
       "2    The gym is dirty and old and the whole place i...   \n",
       "3    Just arrived from the overnight train, arrived...   \n",
       "4    So just in case this is the first review you'v...   \n",
       "..                                                 ...   \n",
       "145  Usually I am not a big stickler for customer s...   \n",
       "146  Of the cheaper casinos on the Strip, Bally's h...   \n",
       "147  Extradinarilly big for a cafe! They've got eve...   \n",
       "148  The serving is good, but the steak dinner is n...   \n",
       "149  We have been here a few times and the food is ...   \n",
       "\n",
       "                                    Tokenized Sentence  \\\n",
       "0    [[CLS], service, and, food, were, awesome, !, ...   \n",
       "1    [[CLS], the, food, was, ok, ,, it, was, kind, ...   \n",
       "2    [[CLS], the, gym, is, dirty, and, old, and, th...   \n",
       "3    [[CLS], just, arrived, from, the, overnight, t...   \n",
       "4    [[CLS], so, just, in, case, this, is, the, fir...   \n",
       "..                                                 ...   \n",
       "145  [[CLS], usually, i, am, not, a, big, stick, ##...   \n",
       "146  [[CLS], of, the, cheaper, casinos, on, the, st...   \n",
       "147  [[CLS], extra, ##dina, ##rill, ##y, big, for, ...   \n",
       "148  [[CLS], the, serving, is, good, ,, but, the, s...   \n",
       "149  [[CLS], we, have, been, here, a, few, times, a...   \n",
       "\n",
       "                                             Input IDs  Number of Tokens  \\\n",
       "0    [101, 2326, 1998, 2833, 2020, 12476, 999, 3811...                23   \n",
       "1    [101, 1996, 2833, 2001, 7929, 1010, 2009, 2001...               163   \n",
       "2    [101, 1996, 9726, 2003, 6530, 1998, 2214, 1998...               279   \n",
       "3    [101, 2074, 3369, 2013, 1996, 11585, 3345, 101...                68   \n",
       "4    [101, 2061, 2074, 1999, 2553, 2023, 2003, 1996...               309   \n",
       "..                                                 ...               ...   \n",
       "145  [101, 2788, 1045, 2572, 2025, 1037, 2502, 6293...               512   \n",
       "146  [101, 1997, 1996, 16269, 27300, 2006, 1996, 61...               185   \n",
       "147  [101, 4469, 18979, 24714, 2100, 2502, 2005, 10...               140   \n",
       "148  [101, 1996, 3529, 2003, 2204, 1010, 2021, 1996...                32   \n",
       "149  [101, 2057, 2031, 2042, 2182, 1037, 2261, 2335...               142   \n",
       "\n",
       "                                 Softmax Probs  Predicted Label  True Label  \\\n",
       "0      [0.2670975625514984, 0.732902467250824]                1           1   \n",
       "1    [0.5392808318138123, 0.46071913838386536]                0           0   \n",
       "2     [0.5794830918312073, 0.4205169379711151]                0           0   \n",
       "3       [0.275282621383667, 0.724717378616333]                1           1   \n",
       "4     [0.4868742823600769, 0.5131257176399231]                1           1   \n",
       "..                                         ...              ...         ...   \n",
       "145   [0.4426237642765045, 0.5573763251304626]                1           1   \n",
       "146   [0.44811585545539856, 0.551884114742279]                1           1   \n",
       "147   [0.4032253921031952, 0.5967746376991272]                1           1   \n",
       "148  [0.5420543551445007, 0.45794567465782166]                0           0   \n",
       "149   [0.5248243808746338, 0.4751756191253662]                0           0   \n",
       "\n",
       "    Sentiment  \n",
       "0    positive  \n",
       "1    negative  \n",
       "2    negative  \n",
       "3    positive  \n",
       "4    positive  \n",
       "..        ...  \n",
       "145  positive  \n",
       "146  positive  \n",
       "147  positive  \n",
       "148  negative  \n",
       "149  negative  \n",
       "\n",
       "[150 rows x 8 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c3720448-ad09-402c-871d-40294d0b667c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Service and food were awesome! Highly recommend the French onion soup. Can't wait to come back.\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df['Sentence'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcdad9e-2681-419f-9d1e-836070d2821b",
   "metadata": {
    "id": "3bcdad9e-2681-419f-9d1e-836070d2821b"
   },
   "source": [
    "### amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf0990-cd57-4fe8-996e-257764bef4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a929ef6c-f245-4048-aa02-1c2ded483581",
   "metadata": {
    "id": "a929ef6c-f245-4048-aa02-1c2ded483581",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeedb257d6f74fa499030302d8e189cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "amazon_data = download_and_select_samples(dataset_name = \"amazon\", n_samples = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8a1320d6-cdf4-489b-b718-56a9219decf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'title', 'content'],\n",
       "    num_rows: 150\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9a6ec88-c062-4e30-9e44-c482c525a951",
   "metadata": {
    "id": "c9a6ec88-c062-4e30-9e44-c482c525a951",
    "outputId": "54525031-4feb-4a48-b0c7-e33fb8a2db0c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labelling Sentences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:17<00:00,  8.47it/s]\n"
     ]
    }
   ],
   "source": [
    "amazon_df, amazon_accuracy= predict_label_dataset(amazon_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ec053b7b-152e-49e7-984e-c9144de77112",
   "metadata": {
    "id": "ec053b7b-152e-49e7-984e-c9144de77112",
    "outputId": "0bae89d5-350f-4a2a-ba53-0f52cfc59a73",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8533333333333334"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a6a7dfe6-6db1-4914-a83b-e7db3beb7bd0",
   "metadata": {
    "id": "a6a7dfe6-6db1-4914-a83b-e7db3beb7bd0",
    "outputId": "6270fc79-26e5-4915-d3bc-24fbd9834927",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tokenized Sentence</th>\n",
       "      <th>Input IDs</th>\n",
       "      <th>Number of Tokens</th>\n",
       "      <th>Softmax Probs</th>\n",
       "      <th>Predicted Label</th>\n",
       "      <th>True Label</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ben Harper was brought to my attention through...</td>\n",
       "      <td>[[CLS], ben, harper, was, brought, to, my, att...</td>\n",
       "      <td>[101, 3841, 8500, 2001, 2716, 2000, 2026, 3086...</td>\n",
       "      <td>132</td>\n",
       "      <td>[0.5244153141975403, 0.47558465600013733]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I think I'm one of the few folks that recieved...</td>\n",
       "      <td>[[CLS], i, think, i, ', m, one, of, the, few, ...</td>\n",
       "      <td>[101, 1045, 2228, 1045, 1005, 1049, 2028, 1997...</td>\n",
       "      <td>60</td>\n",
       "      <td>[0.5591208338737488, 0.44087913632392883]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First, why did I read this book? Do I have an ...</td>\n",
       "      <td>[[CLS], first, ,, why, did, i, read, this, boo...</td>\n",
       "      <td>[101, 2034, 1010, 2339, 2106, 1045, 3191, 2023...</td>\n",
       "      <td>224</td>\n",
       "      <td>[0.4386332631111145, 0.5613666772842407]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if they taught this kind of history in school,...</td>\n",
       "      <td>[[CLS], if, they, taught, this, kind, of, hist...</td>\n",
       "      <td>[101, 2065, 2027, 4036, 2023, 2785, 1997, 2381...</td>\n",
       "      <td>69</td>\n",
       "      <td>[0.48176589608192444, 0.518234133720398]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, well ,well the latest in the o'malley sa...</td>\n",
       "      <td>[[CLS], well, ,, well, ,, well, the, latest, i...</td>\n",
       "      <td>[101, 2092, 1010, 2092, 1010, 2092, 1996, 6745...</td>\n",
       "      <td>66</td>\n",
       "      <td>[0.24162133038043976, 0.7583786249160767]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>I highly recommend Gary Chapman's \"5 Love Lang...</td>\n",
       "      <td>[[CLS], i, highly, recommend, gary, chapman, '...</td>\n",
       "      <td>[101, 1045, 3811, 16755, 5639, 11526, 1005, 10...</td>\n",
       "      <td>71</td>\n",
       "      <td>[0.2884887158870697, 0.7115112543106079]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>I purchased one of the HP 540 series PDA's (in...</td>\n",
       "      <td>[[CLS], i, purchased, one, of, the, hp, 540, s...</td>\n",
       "      <td>[101, 1045, 4156, 2028, 1997, 1996, 6522, 2026...</td>\n",
       "      <td>91</td>\n",
       "      <td>[0.5197734832763672, 0.4802265465259552]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>After reading the many rave reviews, I was exp...</td>\n",
       "      <td>[[CLS], after, reading, the, many, rave, revie...</td>\n",
       "      <td>[101, 2044, 3752, 1996, 2116, 23289, 4391, 101...</td>\n",
       "      <td>192</td>\n",
       "      <td>[0.5477862358093262, 0.45221370458602905]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>For most of my 7th and 8th grade year, a few o...</td>\n",
       "      <td>[[CLS], for, most, of, my, 7th, and, 8th, grad...</td>\n",
       "      <td>[101, 2005, 2087, 1997, 2026, 5504, 1998, 5893...</td>\n",
       "      <td>211</td>\n",
       "      <td>[0.45704326033592224, 0.5429567098617554]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>This has to be RUSH's best work since the earl...</td>\n",
       "      <td>[[CLS], this, has, to, be, rush, ', s, best, w...</td>\n",
       "      <td>[101, 2023, 2038, 2000, 2022, 5481, 1005, 1055...</td>\n",
       "      <td>166</td>\n",
       "      <td>[0.35129478573799133, 0.6487051844596863]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  \\\n",
       "0    Ben Harper was brought to my attention through...   \n",
       "1    I think I'm one of the few folks that recieved...   \n",
       "2    First, why did I read this book? Do I have an ...   \n",
       "3    if they taught this kind of history in school,...   \n",
       "4    Well, well ,well the latest in the o'malley sa...   \n",
       "..                                                 ...   \n",
       "145  I highly recommend Gary Chapman's \"5 Love Lang...   \n",
       "146  I purchased one of the HP 540 series PDA's (in...   \n",
       "147  After reading the many rave reviews, I was exp...   \n",
       "148  For most of my 7th and 8th grade year, a few o...   \n",
       "149  This has to be RUSH's best work since the earl...   \n",
       "\n",
       "                                    Tokenized Sentence  \\\n",
       "0    [[CLS], ben, harper, was, brought, to, my, att...   \n",
       "1    [[CLS], i, think, i, ', m, one, of, the, few, ...   \n",
       "2    [[CLS], first, ,, why, did, i, read, this, boo...   \n",
       "3    [[CLS], if, they, taught, this, kind, of, hist...   \n",
       "4    [[CLS], well, ,, well, ,, well, the, latest, i...   \n",
       "..                                                 ...   \n",
       "145  [[CLS], i, highly, recommend, gary, chapman, '...   \n",
       "146  [[CLS], i, purchased, one, of, the, hp, 540, s...   \n",
       "147  [[CLS], after, reading, the, many, rave, revie...   \n",
       "148  [[CLS], for, most, of, my, 7th, and, 8th, grad...   \n",
       "149  [[CLS], this, has, to, be, rush, ', s, best, w...   \n",
       "\n",
       "                                             Input IDs  Number of Tokens  \\\n",
       "0    [101, 3841, 8500, 2001, 2716, 2000, 2026, 3086...               132   \n",
       "1    [101, 1045, 2228, 1045, 1005, 1049, 2028, 1997...                60   \n",
       "2    [101, 2034, 1010, 2339, 2106, 1045, 3191, 2023...               224   \n",
       "3    [101, 2065, 2027, 4036, 2023, 2785, 1997, 2381...                69   \n",
       "4    [101, 2092, 1010, 2092, 1010, 2092, 1996, 6745...                66   \n",
       "..                                                 ...               ...   \n",
       "145  [101, 1045, 3811, 16755, 5639, 11526, 1005, 10...                71   \n",
       "146  [101, 1045, 4156, 2028, 1997, 1996, 6522, 2026...                91   \n",
       "147  [101, 2044, 3752, 1996, 2116, 23289, 4391, 101...               192   \n",
       "148  [101, 2005, 2087, 1997, 2026, 5504, 1998, 5893...               211   \n",
       "149  [101, 2023, 2038, 2000, 2022, 5481, 1005, 1055...               166   \n",
       "\n",
       "                                 Softmax Probs  Predicted Label  True Label  \\\n",
       "0    [0.5244153141975403, 0.47558465600013733]                0           0   \n",
       "1    [0.5591208338737488, 0.44087913632392883]                0           1   \n",
       "2     [0.4386332631111145, 0.5613666772842407]                1           1   \n",
       "3     [0.48176589608192444, 0.518234133720398]                1           1   \n",
       "4    [0.24162133038043976, 0.7583786249160767]                1           1   \n",
       "..                                         ...              ...         ...   \n",
       "145   [0.2884887158870697, 0.7115112543106079]                1           1   \n",
       "146   [0.5197734832763672, 0.4802265465259552]                0           0   \n",
       "147  [0.5477862358093262, 0.45221370458602905]                0           0   \n",
       "148  [0.45704326033592224, 0.5429567098617554]                1           1   \n",
       "149  [0.35129478573799133, 0.6487051844596863]                1           1   \n",
       "\n",
       "    Sentiment  \n",
       "0    negative  \n",
       "1    negative  \n",
       "2    positive  \n",
       "3    positive  \n",
       "4    positive  \n",
       "..        ...  \n",
       "145  positive  \n",
       "146  negative  \n",
       "147  negative  \n",
       "148  positive  \n",
       "149  positive  \n",
       "\n",
       "[150 rows x 8 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "793fa6a9-3b3a-440a-badc-552f2827ca26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ben Harper was brought to my attention through his association with Jack Johnson. Then Direct TV showed Ben Harper this month on their free concert. I only saw part of the show and decided to buy Live from Mars as my first (and last) Ben Harper CD. I can't get into his music...it doesn't have any flow. His guitar playing is mediocre at best, and his vocals even worse. At times I thought Tiny Tim had come back from the dead. I'll stick with Jack Johnson. Ben Harper was not what I expected, and I utterly fail to see what all the hype is about.\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_df['Sentence'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db2f8c-6e95-4d3d-a014-002e5db8070a",
   "metadata": {},
   "source": [
    "# Comparing tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "faf92fcd-2c4f-4124-add8-105fd225d971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the tokenizers to be compared\n",
    "tokenizers = {\n",
    "    \"BART\": \"facebook/bart-base\",\n",
    "    \"DistilBERT\": \"distilbert-base-uncased\",\n",
    "    \"GPT-2\": \"gpt2\",\n",
    "    \"T5\": \"t5-small\",\n",
    "    \"Albert\": \"albert-base-v2\",\n",
    "    \"XLM-Roberta\": \"xlm-roberta-base\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6d72a660-c06f-4f85-b0dc-1c34414373fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_tokenizer(model_name):\n",
    "    \"\"\"\n",
    "    Load and return a tokenizer based on the provided model name.\n",
    "    \"\"\"\n",
    "    return AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7876fc25-74fb-4f92-bbe6-82a1b488aab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_vocab_size(tokenizer):\n",
    "    \"\"\"\n",
    "    Return the vocabulary size of the provided tokenizer.\n",
    "    \"\"\"\n",
    "    return tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "928f33de-9673-48b9-a770-528b85fcc13a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_sentence(tokenizer, sentence):\n",
    "    \"\"\"\n",
    "    Tokenize the sentence using the provided tokenizer and return the tokens and token IDs.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    token_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    return tokens, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "985620a7-65fe-46d1-9b34-1c934b995aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_tokenizer_info(tokenizer_name, tokens, token_ids, vocab_size):\n",
    "    \"\"\"\n",
    "    Print information about the tokenizer, including tokens, token IDs, and vocabulary size.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{tokenizer_name} Vocabulary Size: {vocab_size}\")\n",
    "    print(f\"{tokenizer_name} Tokenized Sentence:\")\n",
    "    print(tokens)\n",
    "    print(f\"{tokenizer_name} Token IDs:\")\n",
    "    print(token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "616fa702-22cf-46f1-91b3-a9901cbb0fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the tokenizers to be compared\n",
    "tokenizers = {\n",
    "    \"BART\": \"facebook/bart-base\",\n",
    "    \"DistilBERT\": \"distilbert-base-uncased\",\n",
    "    \"GPT-2\": \"gpt2\",\n",
    "    \"T5\": \"t5-small\",\n",
    "    \"Albert\": \"albert-base-v2\",\n",
    "    \"XLM-Roberta\": \"xlm-roberta-base\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e520c2b0-7d7f-4d48-be42-88991bc0e910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sentence to tokenize\n",
    "sentence = \"This is how a tokenized expression looks. En espaÃ±ol es distinto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fb818940-05ae-4036-93cf-cc0492d3afcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BART Vocabulary Size: 50265\n",
      "BART Tokenized Sentence:\n",
      "['This', 'Ä is', 'Ä how', 'Ä a', 'Ä token', 'ized', 'Ä expression', 'Ä looks', '.', 'Ä En', 'Ä es', 'pa', 'ÃƒÂ±', 'ol', 'Ä es', 'Ä dist', 'into']\n",
      "BART Token IDs:\n",
      "[0, 713, 16, 141, 10, 19233, 1538, 8151, 1326, 4, 2271, 2714, 6709, 6303, 1168, 2714, 7018, 12473, 2]\n",
      "\n",
      "DistilBERT Vocabulary Size: 30522\n",
      "DistilBERT Tokenized Sentence:\n",
      "['this', 'is', 'how', 'a', 'token', '##ized', 'expression', 'looks', '.', 'en', 'es', '##pan', '##ol', 'es', 'di', '##sti', '##nto']\n",
      "DistilBERT Token IDs:\n",
      "[101, 2023, 2003, 2129, 1037, 19204, 3550, 3670, 3504, 1012, 4372, 9686, 9739, 4747, 9686, 4487, 16643, 13663, 102]\n",
      "\n",
      "GPT-2 Vocabulary Size: 50257\n",
      "GPT-2 Tokenized Sentence:\n",
      "['This', 'Ä is', 'Ä how', 'Ä a', 'Ä token', 'ized', 'Ä expression', 'Ä looks', '.', 'Ä En', 'Ä es', 'pa', 'ÃƒÂ±', 'ol', 'Ä es', 'Ä dist', 'into']\n",
      "GPT-2 Token IDs:\n",
      "[1212, 318, 703, 257, 11241, 1143, 5408, 3073, 13, 2039, 1658, 8957, 12654, 349, 1658, 1233, 20424]\n",
      "\n",
      "T5 Vocabulary Size: 32100\n",
      "T5 Tokenized Sentence:\n",
      "['â–This', 'â–is', 'â–how', 'â–', 'a', 'â–token', 'ized', 'â–expression', 'â–looks', '.', 'â–En', 'â–esp', 'a', 'Ã±', 'o', 'l', 'â–', 'e', 's', 'â–', 'distin', 'to']\n",
      "T5 Token IDs:\n",
      "[100, 19, 149, 3, 9, 14145, 1601, 3893, 1416, 5, 695, 16159, 9, 2, 32, 40, 3, 15, 7, 3, 19694, 235, 1]\n",
      "\n",
      "Albert Vocabulary Size: 30000\n",
      "Albert Tokenized Sentence:\n",
      "['â–this', 'â–is', 'â–how', 'â–a', 'â–to', 'ken', 'ized', 'â–expression', 'â–looks', '.', 'â–en', 'â–espanol', 'â–', 'es', 'â–dis', 'tin', 'to']\n",
      "Albert Token IDs:\n",
      "[2, 48, 25, 184, 21, 20, 2853, 1333, 1803, 1879, 9, 1957, 24339, 13, 160, 1460, 2864, 262, 3]\n",
      "\n",
      "XLM-Roberta Vocabulary Size: 250002\n",
      "XLM-Roberta Tokenized Sentence:\n",
      "['â–This', 'â–is', 'â–how', 'â–a', 'â–to', 'ken', 'ized', 'â–expression', 'â–looks', '.', 'â–En', 'â–espaÃ±ol', 'â–es', 'â–distin', 'to']\n",
      "XLM-Roberta Token IDs:\n",
      "[0, 3293, 83, 3642, 10, 47, 1098, 29367, 125195, 33342, 5, 357, 36131, 198, 34973, 188, 2]\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers and compare them\n",
    "for name, model_name in tokenizers.items():\n",
    "    tokenizer = load_tokenizer(model_name)\n",
    "    vocab_size = get_vocab_size(tokenizer)\n",
    "    tokens, token_ids = tokenize_sentence(tokenizer, sentence)\n",
    "    print_tokenizer_info(name, tokens, token_ids, vocab_size)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "32cc46a7-b9f3-496f-8c1d-7c6700e68edd"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".conda-default:Python",
   "language": "python",
   "name": "conda-env-.conda-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
